\section{Week 2 -- Linear regression}

The linear regreession consists in finding best fit parameters of a linear function that will be used as a model 
or hypothesis. The mathematical expression is:

\begin{equation} \label{h}
    h_{\theta}(x) = \theta_{0} + \theta_{1} x
\end{equation}
This is the case of a one dimensional model i.e. y is a function of one variable. In machine learning language, the variables will be called features.
It is very convinient, specially from the point of view of code implementation, to write this function in vectorial form. 
The parameters $\theta_{j}$ are written as a single column vector $\theta = (\theta_{0}, \theta_{1})$. The feature is written in the vector form by using the trick
of adding a dimension with a unit value representing $x^{0}$: $X = (x_{0}=1, x_{1})$. The hypothesis \ref{h} is then simplified:

\begin{equation}
    h_{\theta}(x) = X \cdot \theta
\end{equation}
Recalling that dim($X$) =  $m \times (n+1)$ matrix (assemble of column vectors) and $\theta$ is a column vector, dim($\theta$) = $(n+1) \times 1$,
where $m$ is the number of training examples and $n$ is the number of variables or features.
The task of the linear regression method is to, given a training data set $(x,y)$, find the best parameters $\theta$ that will fit the model to the data.
Based on this, one can use the output of the calculation to predict or to estimate $y$. The search for the optimum parameters can be done by writing the function:

\begin{equation}
    \begin{split}
        J(\theta) &= \frac{1}{2 m} \sum _{k=1}^{m} [ h_{\theta}(x^{(k)}) - y^{(k)}]^{2} \\
                    & = \frac{1}{2 m} (X \cdot \theta - y)^{T} \cdot (X \cdot \theta - y)
    \end{split}
\end{equation}
called the cost function. It is proportional to the square of the difference between the model and the trainning data set. 
The last step is to find a routine that can minimize the cost function. One way to do this is to use the gradient descendent 
method by calculating in an iterative manner the parameters using the equation:

\begin{equation} \label{grad_desc}
    \theta_{i} = \theta_{i} - \alpha \frac{\partial J}{\partial \theta}
\end{equation}
where $\alpha$, the learning rate, is a parameter to be adjusted and will control the convergence speed of the algorithm. 
Values of $\alpha$ that are too large will make the algorithm to overshoot and never reach the minimum
value while a too small $\alpha$ may slow the code down. An optimum value must be found. It is worth mentioning that the operator "="
is being used here as an assignment operator and not truth assertion.

Equation \ref{grad_desc} can be explicitly calculated:

\begin{equation}
    \begin{split}
        \theta_{i} & = \theta_{i} - \frac{\alpha}{m} \sum _{k=1}^{m} [ h_{\theta}(x^{(k)}) - y^{(k)}] x_{i}^{(k)} \\
                   \theta & = \theta - \frac{\alpha}{m} X^{T} \cdot (X \cdot \theta - y)
    \end{split}
\end{equation}

Writting the equations in the vector form allows us to calculate the updated values in a simultaneous way for all parameters,
which is the desired routine. From the coding point of view, the summation form requires the use of a iterative for loop over the 
vectors and matrices (and one needs to be carefull with ensuring the simultaneous update of the parameters) while the vector form is a simple single line of code (simultaneous by definition).

For a multi-dimensional problems i.e. with more than one feature, in the vectorial form, the problem is exact the same and no change need 
to be made in the equations. The expression for the hypothesis is modified as:

\begin{equation}
    \begin{split}
        h_{\theta}(x) & = \sum_{i=1}^{n} \theta_{i} x_{i} \\
                        & = X \cdot \theta
    \end{split}
\end{equation}
with $x_{0} = 1$.